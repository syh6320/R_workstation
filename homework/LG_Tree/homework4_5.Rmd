---
title: "homework4_5"
author: "syh"
date: "June 2, 2017"
output: pdf_document
---

```{r}
# in this section, we would like to use xgboost to predict sale price
# attention: only matrix and factor are supported in xgboost
```

```{r}
# read all data (train + prediction) without missing value
real_all_data <- read.csv(file = "H:/kaggle/houseprice/data/real_all_data_hybrid.csv",
                          stringsAsFactors = FALSE)[,-c(1,2)]
```

```{r}
# transform sale price to log sale price
real_all_data[,"SalePrice"] <- log(real_all_data[,"SalePrice"])
```

```{r}
# 1. convert categorical ones to factors
for(i in 1:dim(real_all_data)[2]){
  if(is.character(real_all_data[,i])){
    real_all_data[,i] <- as.factor(real_all_data[,i])
  }
}
```

```{r}
# 2. convert independent features into matrix
fea_mat <- model.matrix(~., data = subset(real_all_data, select = -SalePrice))
```

```{r}
# 1. split all data into train and prediction

model_x <- fea_mat[1:1460,]
model_y <- real_all_data[1:1460, "SalePrice"]

pre_x <- fea_mat[-c(1:1460),]

# 2. split model data into train and test
# seed <- sample.int(n = 10000, size = 1)
# set.seed(seed)
set.seed(2)
train_ind <- sample(1:dim(model_x)[1], size = dim(model_x)[1] * 0.7)

train_x <- model_x[train_ind,]
train_y <- model_y[train_ind]

test_x <- model_x[-train_ind,]
test_y <- model_y[-train_ind]
```


```{r}
# at first we use common parameter to train a simple xgboost model
library(xgboost)

set.seed(2)
xgb <- xgboost(data = train_x, label = train_y, nrounds = 20, 
               verbose = 0, nthread = 3, max_depth = 8, object = "reg:linear"
               )
```

```{r}
# let's look at importance of each featues
importance <- xgb.importance(feature_names = colnames(train_x), model = xgb)
importance

```
```{r}
# visually look at first 15 most important features
library(ggplot2)
library(Ckmeans.1d.dp)
xgb.ggplot.importance(importance_matrix = importance, top_n = 15)
```

```{r}
# let's calculate the sse of this model on test data
est_y <- predict(object = xgb, newdata = test_x)
see <- sum((est_y - test_y) ^ 2)
see
# it seems that it's even worse than random forest, linear regression
```


```{r}
# try to use xgb.cv, cross validation, to find optimal combination of parameters.
# set possible combinations of parameters
library(caret)
paraGrid <- expand.grid(
            nrounds = c(100,200),
            max_depth = c(5, 10, 15),
            eta = c(0.01,0.1, 0.3),
            gamma = 0,  # first 0
            min_child_weight = c(5, 10, 15),
            subsample = c(.5, 0.6, 0.7, .8),
            colsample_bytree = c(0.5, 0.6, 0.7, 0.8)
            )

```

```{r}
# set train control
xgb_Control <- trainControl(method = "repeatedcv", verboseIter = F, 
                            number = 5,repeats = 2, 
                            returnData = FALSE, allowParallel = TRUE
                          )
```


```{r}
# trian our model using cross validation for grid search for parameters
gbm_train <- train(x = train_x, y = train_y, method = "xgbTree", 
                   trControl = xgb_Control, tuneGrid = paraGrid)
```


```{r}
rmse_order <- order(gbm_train$results$RMSE)
head(gbm_train$result[rmse_order,])
```

```{r}
# find the best parameter here is eta = 0.1, max_depth = 5, gmma = 0,
# colsmaple_bytree = 0.8(maximum), min_child_weight = 15(which is maximum),
# subsample = 0.8(is maximum),  nround = 200(maximum)
# these maximum parameters should be adjusted again
```

```{r}
# let's make a estimate on test data
est_y <- predict(object = gbm_train, newdata = test_x)
sqrt(mean((est_y - test_y) ^ 2))
```

```{r}
# we find that difference between est_y and minimum rmse, 0.1264137, is large
# so we should also tune parameter gamma
```

```{r}
# run a more precise tune 
paraGrid1 <- expand.grid(
            nrounds = c(200, 250, 300),
            max_depth = 5,
            eta = c(0.05,0.1, 0.2),
            gamma = 0,
            min_child_weight = c(15,20,30),
            subsample = c(0.7, .8, 0.9),
            colsample_bytree = c(0.8, 0.9)
            )
```

```{r}
gbm_train1 <- train(x = train_x, y = train_y, method = "xgbTree", 
                   trControl = xgb_Control, tuneGrid = paraGrid1)
```

```{r}
gbm_train1$bestTune
```

```{r}
# for now best parameters
# nround = 300(maximum), max_depth = 5, eta = 0.05, gamma = 0
# colsample_bytree = 0.8,
# subsample = 0.7
# min_child_weight = 15
```

```{r}
rmse_order1 <- order(gbm_train1$results$RMSE)
head(gbm_train1$results[rmse_order1,])
```

```{r}
est_y1 <- predict(object = gbm_train1, newdata = test_x)
sqrt(mean((est_y1 - test_y)^2))
```

```{r}
# make prediction
pre_sale_price <- predict(object = gbm_train1, newdata = pre_x)
result <- data.frame(Id = c(1461:2919), SalePrice = exp(pre_sale_price))
write.csv(x = result, file = "H:/kaggle/houseprice/data/submission_7.csv",
          row.names = FALSE)
```

