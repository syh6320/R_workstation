---
title: "homework3"
author: "syh"
date: "May 22, 2017"
output: pdf_document
---

```{r}
# get data
raw_train <- read.csv(file = "H:/kaggle/houseprice/data/train.csv", 
                      stringsAsFactors = FALSE)
```

```{r}
# get hang of raw data
str(raw_train)
```

```{r}
# deal with NA value
# first have a look which columns have NAs
na_sort <- sort(sapply(raw_train, function(x){
  sum(is.na(x))
}), decreasing = TRUE)

na_sort
```

```{r}
# 1. just now we delete columns with too many NAs
keep_col <- names(which(na_sort < dim(raw_train)[1] * 0.05))

train_no_large_na <- raw_train[,keep_col]
```

```{r}
# 2. find catergocial and numerical features with NA respectively.
cat_na_col <- which(sapply(train_no_large_na, function(x){
  sum(is.na(x)) > 0 & !is.numeric(x)
}))
cat_na_col
num_na_col <- which(sapply(train_no_large_na, function(x){
  sum(is.na(x)) > 0 & is.numeric(x)
}))
num_na_col
```

```{r}
# 2. Missingness is caused by that it doesn't exsit
train_no_large_na[is.na(train_no_large_na$BsmtExposure),cat_na_col]
```

```{r}
train_no_large_na$BsmtExposure[which(is.na(train_no_large_na$BsmtExposure))] <- 'Unf'
train_no_large_na$BsmtFinType1[which(is.na(train_no_large_na$BsmtFinType1))] <- 'Unf'
train_no_large_na$BsmtFinType2[which(is.na(train_no_large_na$BsmtFinType2))] <- 'Unf'
train_no_large_na$BsmtQual[which(is.na(train_no_large_na$BsmtQual))] <- 'Unf'
train_no_large_na$BsmtCond[which(is.na(train_no_large_na$BsmtCond))] <- 'Unf'
```

```{r}
with(subset(train_no_large_na, subset = is.na(MasVnrType)), summary(MasVnrArea))
```

```{r}
# MasVnrAreas are all na when type is na, 
# so it doesn't exist
train_no_large_na$MasVnrType[which(is.na(train_no_large_na$MasVnrType))] <- 'None'
```

```{r}
# Simple missing might due to operation or data transfer, mean, median, etc
train_no_large_na$MasVnrArea[which(is.na(train_no_large_na$MasVnrArea))] <- mean(train_no_large_na$MasVnrArea, na.rm = TRUE)
# area is set 0 where type is None
train_no_large_na$MasVnrArea[which(train_no_large_na$MasVnrType == "None")] <- 0
```

```{r}
# for exectrical, this is only one missing value
# so we use most frequent value in this column to impute it.
train_no_large_na$Electrical[is.na(train_no_large_na$Electrical)] <- "SBrkr"
barplot(table(train_no_large_na$Electrical))
```

```{r}
# check again if there is no missing value
sort(sapply(train_no_large_na, function(x){sum(is.na(x))}), decreasing = TRUE)
# there isn't missing value any more.
```


```{r}

# train_no_miss <- train_no_large_na
# record this train data set
write.csv(file = "H:/kaggle/houseprice/data/train_no_missing.csv", x = train_no_large_na)

```

```{r}
#create whole train data without id
whole_train <- subset(train_no_large_na, select = -Id)
```


```{r}
# adding new features, feature engineering
# how many years are these houses
train_no_miss$Age <- 2017 - train_no_miss[,"YearBuilt"]

# total Floor square feet + basement
train_no_miss$tot_Flo_area <- train_no_miss$X1stFlrSF 
                + train_no_miss$X2ndFlrSF 
                + train_no_miss$TotalBsmtSF

# how many years house last since repairing
train_no_miss$rep_yea <- 2017 - train_no_miss$YearRemodAdd
```


```{r}
# transform sale price to more normal, 
# in order to subject to assumptin of linear regression
whole_train$SalePrice <- log(whole_train$SalePrice)
```


```{r}
# train a simple linear regression model first
simple_lm <- lm(SalePrice ~ ., data = whole_train)
summary(simple_lm)
```

```{r}
# residual plot
# 1st: residual is unbais and homoscedastic, 
# 2nd: basically, residual is subject to normal distribution which means variacne of error is constant.
# 4th: we know some outlier: 633, 826, 524 high leaverage and high residual
plot(simple_lm)
```

```{r}
# we can draw conclusion that basically underlying relations is linear
# Adjusted R-squared:  0.9358, it seems it's pretty good.
# F-statistic: 93.07, it's not very high, though.
```


```{r}
# transform categorical features into factor
fac_col <- c("MSSubClass", "MSZoning", "Street", "LotShape", "LandContour","Utilities", "LotConfig", "LandSlope", "Neighborhood", "Condition1", "Condition2","BldgType", "HouseStyle", "OverallQual", "OverallCond", "RoofStyle", "RoofMatl","Exterior1st", "Exterior2nd", "MasVnrType", "ExterQual", "ExterCond", "Foundation","BsmtQual", "BsmtCond", "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "Heating","HeatingQC", "CentralAir", "Electrical", "KitchenQual", "Functional", "PavedDrive", 
"MoSold", "SaleType", "SaleCondition")

whole_train[fac_col] <- sapply(whole_train[fac_col], as.factor)
```


```{r}
# scale numerical features
# for purpose of comparing predection power of different features
num_col <- setdiff(colnames(whole_train),fac_col)
whole_train[setdiff(num_col,"SalePrice")] <- sapply(whole_train[setdiff(num_col,"SalePrice")], scale)
```


```{r}
# split data into train(0.7) and test(0.3)
dep_data <- whole_train$SalePrice
# x must be a matrix for glmnet
ind_data <- model.matrix(~., subset(whole_train, select = -SalePrice))

set.seed(1000)
train_ind <- sample(x = 1:dim(whole_train)[1], size = dim(whole_train)[1] * 0.7)

x_train <- ind_data[train_ind,]
y_train <- dep_data[train_ind]

x_test <- ind_data[-train_ind,]
y_test <- dep_data[-train_ind]
```



```{r}
# resolve multicolinearity and select lamda
# we choose different penality methods
library(glmnet)
lasso_lm <- glmnet(x = x_train, y = y_train, alpha = 1)
ridge_lm <- glmnet(x = x_train, y = y_train, alpha = 0)
elnet_lm <- glmnet(x = x_train, y = y_train, alpha = 0.5)
```

```{r}
for(i in c(0:10)){
  assign(paste("cvglm", i,sep = ""), 
         cv.glmnet(x = x_test, y = y_test, alpha = i/10,
                  type.measure = "mse", family = "gaussian"))
}
```


```{r}
par(mfrow=c(1,2))
plot(lasso_lm, xvar = "lambda", label = T)
plot(cvglm10)

```


```{r}
# let calculate the mse of these three models
lasso_y <- predict.glmnet(object = lasso_lm, newx = x_test, s = cvglm10$lambda.1se)
mean((y_test - lasso_y)^2)
```


```{r}
par(mfrow=c(1,2))
plot(ridge_lm, xvar = "lambda", label = T)
plot(cvglm0)
```

```{r}
ridge_y <- predict.glmnet(object = ridge_lm, newx = x_test, s = cvglm0$lambda.1se)
mean((y_test - ridge_y)^2)
```


```{r}
elnet_y <- predict.glmnet(object = elnet_lm, newx = x_test, s = cvglm5$lambda.1se)
mean((y_test - elnet_y)^2)
```


```{r}
cvglm <- list(cvglm0,cvglm1,cvglm2,cvglm3,cvglm4,cvglm5,
           cvglm6,cvglm7,cvglm8,cvglm9,cvglm10
           )
mse <- NULL
for(i in c(1:11)){
  y_pre <- predict.cv.glmnet(object = cvglm[[i]],newx = x_test,
                             s = cvglm[[i]]$lambda.1se)
  mse <- c(mse, mean((y_test - y_pre)^2))
}
```

```{r}
plot(x = c(0:10), y = mse, xlab = "alpha", ylab = "mse")
```

```{r}
which.min(mse) - 1
```


